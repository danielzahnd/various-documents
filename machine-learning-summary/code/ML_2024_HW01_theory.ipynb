{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b4cc74",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518781b8-6d90-494c-99dd-ddd5ffaa90a8",
   "metadata": {},
   "source": [
    "## Task 1. Locally Weighted Linear Regression [6 points]\n",
    "\n",
    "Under the assumption that $\\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(0)}) \\mathbf{X}$ is invertible, derive the closed form solution for the LWR problem, defined in Task 3 of the practical part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d699613-ed8c-4304-a85f-b2b9a7735a47",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c36ff",
   "metadata": {},
   "source": [
    "In LWR a separate regression is fitted and used to predict the outcome at each query poins. The training examples are weighted according to their similarity to the query point (in the simplest form, the distance to the query point is used). That is, for a data point $\\mathbf{x}^{(j)} \\in \\mathbb{R}^d$ the prediction is given by \n",
    "\n",
    "$$\n",
    "\\hat y^{(j)} = \\theta^*(\\mathbf{x}^{(j)})^\\top \\mathbf{x}^{(j)},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta^*(\\mathbf{x}^{(j)}) = \\arg \\min_{\\theta(\\mathbf{x}^{(j)})} \\sum_{i = 1}^m w^{(i)}(\\mathbf{x}^{(j)}) \\left(y^{(i)} - \\theta(\\mathbf{x}^{(j)})^\\top \\mathbf{x}^{(i)}\\right)^2.\n",
    "$$\n",
    "\n",
    "Let $\\mathbf{W}(\\mathbf{x}^{(j)})$ be a $m\\times m$ diagonal matrix with $w^{(i)}(\\mathbf{x}^{(j)})$ on the $i$-th diagonal element; and let the vector $\\mathbf{y}$ be given by $\\mathbf{y}^\\top = (y^{(1)}, \\dots , y^{(m)})$. An element $w^{(i)}(x^{(0)})$ of the weights matrix is given by\n",
    "$$\n",
    "w^{(i)}(\\mathbf{x}^{(0)}) = \\exp\\left(- \\frac{\\|\\mathbf{x}^{(0)} - \\mathbf{x}^{(i)}\\|^2}{2 \\tau^2}\\right).\n",
    "$$\n",
    "\n",
    "One can define the argument in the $\\arg\\max$ expression as \n",
    "$$\n",
    "F(\\theta; \\mathbf{x}^{(j)}) \\doteq \\sum_{i = 1}^m w^{(i)}(\\mathbf{x}^{(j)}) \\left(y^{(i)} - \\theta(\\mathbf{x}^{(j)})^\\top \\mathbf{x}^{(i)}\\right)^2.\n",
    "$$\n",
    "\n",
    "Defining\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^\\top \\doteq ((\\mathbf{x}^{(1)})^\\top,\\dots,(\\mathbf{x}^{(m)})^\\top),\n",
    "$$ \n",
    "\n",
    "$F(\\theta; \\mathbf{x}^{(j)})$ can be rewritten in matrix form as\n",
    "\n",
    "\\begin{align*}\n",
    "F(\\theta; \\mathbf{x}^{(j)}) &= (\\mathbf{\\mathbf{y}}-\\mathbf{\\mathbf{X}}\\theta)^\\top \\mathbf{\\mathbf{W}(\\mathbf{x}^{(j)})}(\\mathbf{x}^{(j)})(\\mathbf{\\mathbf{y}}-\\mathbf{\\mathbf{X}}\\theta) \\\\\n",
    "&= \\mathbf{y}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) - \\theta^\\top \\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)})(\\mathbf{y}-\\mathbf{X}\\theta)\\\\\n",
    "&= \\mathbf{y}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{y} - \\underbrace{\\theta^\\top \\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{y}}_{\\text{scalar}} - \\underbrace{\\mathbf{y}^\\top \\mathbf{W}(\\mathbf{x}^{(j)})\\mathbf{X} \\theta}_{\\text{scalar}} + \\theta^\\top \\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{X} \\theta \\\\ \n",
    "&= \\mathbf{y}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{y} -2\\mathbf{y}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{X} \\theta + \\theta^\\top \\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{X} \\theta.\n",
    "\\end{align*}\n",
    "\n",
    "Taking the gradient $\\nabla_\\theta$ of this expression, one obtains\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta F(\\theta; \\mathbf{x}^{(j)}) &= -2(\\mathbf{y}^\\top \\mathbf{W}( \\mathbf{x}^{(j)})\\mathbf{X})^\\top + 2\\mathbf{X}^\\top \\mathbf{W}( \\mathbf{x}^{(j)})\\mathbf{X}\\theta \\\\\n",
    "&= -2\\mathbf{X}^\\top \\mathbf{W}( \\mathbf{x}^{(j)})^\\top \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{W}( \\mathbf{x}^{(j)})\\mathbf{X}\\theta \\\\\n",
    "&= -2\\mathbf{X}^\\top \\mathbf{W}( \\mathbf{x}^{(j)}) \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{W}( \\mathbf{x}^{(j)})\\mathbf{X}\\theta.\n",
    "\\end{align*}\n",
    "\n",
    "Setting this gradient to zero, one can find the optimal model parameters $\\theta^*(\\mathbf{x}^{(j)})$. Setting the gradient to zero and rearranging - by means of assuming that $\\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{X}$ is invertible - leads to \n",
    "$$\n",
    "\\theta^*(\\mathbf{x}^{(j)}) = (\\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)})\\mathbf{y}.\n",
    "$$ \n",
    "Given that a matrix of shape $(k,l)$ times a matrix of shape $(l,n)$ is of shape $(k,n)$ and given that $\\mathbf{X}$ is of shape $(m, d)$, $\\mathbf{W}(\\mathbf{x}^{(j)})$ is of shape $(m, m)$ and $\\mathbf{y}$ is of shape $(d, 1)$, the resulting vector $\\theta^*(\\mathbf{x}^{(j)})$ is of shape $(d, 1)$. This means that in order to obtain the prediction $\\hat{y}^{(j)}$ on sample $\\mathbf{x}^{(j)}$, we need to calculate \n",
    "$$\n",
    "\\hat{y}^{(j)} = (\\mathbf{x}^{(j)})^\\top \\theta^*(\\mathbf{x}^{(j)}) = (\\mathbf{x}^{(j)})^\\top(\\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)}) \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{W}(\\mathbf{x}^{(j)})\\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2a8c7",
   "metadata": {},
   "source": [
    "## Task 2. Multiclass Naive Bayes Classifier [4 points]\n",
    "\n",
    "Let us consider **multiclass classification problem** with classes $C_1, \\dots, C_K$.\n",
    "\n",
    "Assume that all $d$ features $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$ are **binary**, i.e. $x_{i} \\in \\{0, 1\\}$ for $i = \\overline{1, d}$ **or** feature vector $\\mathbf{x} \\in \\{0, 1\\}^d$.\n",
    "\n",
    "Show that the decision rule of a **Naive Bayes Classifier** can be represented as $\\arg\\max$ of linear functions of the input.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Hint**: use the **maximum a posteriori** (MAP) decision rule: $\\hat{y} = \\arg\\max\\limits_{y \\in \\overline{1, K}} p(y)p(\\mathbf{x}|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372e710",
   "metadata": {},
   "source": [
    "### Your solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490b50d",
   "metadata": {},
   "source": [
    "Given the IID assumption and general properties of probabilities, we can write the probability $p(\\mathbf{x}|y)$ as\n",
    "$$\n",
    "p(\\mathbf{x}|y) = p(x_1,\\dots,x_d|y) = \\prod_{i=1}^{d}p(x_i|y).\n",
    "$$\n",
    "Since $x_i \\in \\{0,1\\}$, we can split each probability $p(x_i|y)$ as $p(x_i|y) = p(x_i=1|y)^{x_i}p(x_i=0|y)^{1-x_i}$. Inserting this into the above expression yields\n",
    "$$\n",
    "p(\\mathbf{x}|y) = \\prod_{i=1}^{d}p(x_i=1|y)^{x_i}p(x_i=0|y)^{1-x_i}.\n",
    "$$\n",
    "Now, the $\\arg\\max$ of $p(\\mathbf{x}|y)$ with respect to $y$ does not depend on the amplitude of the probability, only on its location with respect to $y$. This means, that we can take the (natural) logarithm of the above expression and write\n",
    "\\begin{align*}\n",
    "\\arg\\max_y \\left[p(y|\\mathbf{x})\\right] &= \\arg\\max_y \\left[p(y)p(\\mathbf{x}|y)\\right] = \\arg\\max_y \\left(\\ln\\left[p(y)p(\\mathbf{x}|y)\\right]\\right) \\\\\n",
    "&= \\ln\\left[p(y)\\right] + \\sum_{i=1}^d x_i\\ln\\left[p(x_i=1|y)\\right] + \\sum_{i=1}^{d} (1-x_i)\\ln\\left[p(x_i=0|y)\\right] \\\\\n",
    "&= \\ln\\left[p(y)\\right] + \\sum_{i=1}^{d}\\ln\\left[p(x_i=0|y)\\right] + \\sum_{i=1}^{d}x_i\\ln\\left[\\frac{p(x_i=1|y)}{p(x_i=0|y)}\\right].\n",
    "\\end{align*} \n",
    "Defining now $\\phi_y \\doteq p(y)$, $\\phi_{i=1,y} \\doteq p(x_i=1|y)$ and $\\phi_{i=0,y} \\doteq p(x_i=0|y)$ where $\\phi_{i=0,y} = 1-\\phi_{i=1,y}$, one can write the above equation in simpler terms as \n",
    "\\begin{equation*}\n",
    "\\arg\\max_y \\left[p(y|\\mathbf{x})\\right] =\\ln\\left[\\phi_y\\right] + \\sum_{i=1}^{d}\\ln\\left[\\phi_{i=0,y}\\right] + \\sum_{i=1}^{d}x_i\\ln\\left[\\frac{\\phi_{i=1,y}}{\\phi_{i=0,y}}\\right].\n",
    "\\end{equation*} \n",
    "Furthermore defining $b_y \\doteq \\ln\\left[\\phi_y\\right] + \\sum_{i=1}^{d}\\ln\\left[\\phi_{i=0,y}\\right]$ and $w_{i,y} \\doteq \\ln\\left[\\frac{\\phi_{i=1,y}}{\\phi_{i=0,y}}\\right]$, the classifier can be written as \n",
    "\\begin{equation*}\n",
    "\\hat{y} = \\arg\\max_y \\left[p(y|\\mathbf{x})\\right]  = b_y + \\sum_{i=1}^{d}x_iw_{i,y},\n",
    "\\end{equation*} \n",
    "which proves that the naive Bayes classifier can be written as a linear (affine) function of the input $\\mathbf{x}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
