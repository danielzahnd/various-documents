{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c41381",
   "metadata": {},
   "source": [
    "# Home Assignment No. 3: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49fd8c",
   "metadata": {},
   "source": [
    "## Task 1. Ensembling Method [4 points]\n",
    "\n",
    "Suppose that we have random variables $X_i$ for $0 \\leq i \\leq n-1$ with $\\mathbb{V}(X_i) = \\sigma_i^2$. Moreover, any $X_k$ and $X_l$ are correlated by a factor of ${\\rho}_{kl}$ for any $k$ and $l$. Calculate the variance of the average of these random variables as in $Z = \\frac{1}{n}\\sum\\limits_{i=0}^{n-1}X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7c0a9",
   "metadata": {},
   "source": [
    "Note first, that the correlation coefficient $\\rho_{kl}$ is defined by \n",
    "$$\n",
    "\\rho_{kl} = \\frac{\\mathbb{K}(X_k,X_l)}{\\sqrt{\\mathbb{V}(X_k)\\mathbb{V}(X_l)}}, \\qquad \\mathbb{K}(X_k,X_l) = \\mathbb{E}\\left[(X_k-\\mathbb{E}[X_k])(X_l-\\mathbb{E}[X_l])\\right].\n",
    "$$\n",
    "We thus calculate\n",
    "\\begin{align*}\n",
    "\\mathbb{V}(Z) &= \\mathbb{E}[(Z-\\mathbb{E}[Z])^2] = \\mathbb{E}\\left[\\left(\\frac{1}{n}\\sum_{i=0}^{n-1}X_i - \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=0}^{n-1}X_i\\right]\\right)^2\\right] \\\\\n",
    "&= \\mathbb{E}\\left[\\left(\\frac{1}{n}\\sum_{i=0}^{n-1}X_i - \\mathbb{E}[X_i]\\right)^2\\right] =  \\frac{1}{n^2}\\sum_{k=0}^{n-1}\\sum_{l=0}^{n-1}\\mathbb{E}\\left[(X_k-\\mathbb{E}[X_k])(X_l-\\mathbb{E}[X_l])\\right] \\\\\n",
    "&= \\frac{1}{n^2}\\sum_{k=0}^{n-1}\\sum_{l=0}^{n-1}\\mathbb{K}(X_k,X_l) = \\frac{1}{n^2}\\sum_{k=0}^{n-1}\\sum_{l=0}^{n-1}\\rho_{kl}\\sigma_k\\sigma_l.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12288a6f",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Minimizing the loss function is an optimization task, and **\"Gradient Boosting\" is one of many methods to perform optimization**. It uses a greedy approach and, therefore, may produce results that are not _globally_ optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & G_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\alpha_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & f_N(x) = \\sum_{n=0}^N \\alpha_n (x) G_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss function $\\mathcal{L}(y, \\hat{y})$ for a target $y$ and a prediction $\\hat{y}$, and let\n",
    "$(x_i, y_i)_{i = 1}^m$ be our train dataset for the regression task. \n",
    "\n",
    "\n",
    "1. Initialize $f_0(x) = z$ with the _flat constant prediction_\n",
    "\n",
    "$$z = \\arg\\min\\limits_{\\hat{y} \\in \\mathbb{R}} \\sum\\limits_{i = 1}^m \\mathcal{L}(y_i, \\hat{y})$$\n",
    "\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $L_n(G_n, \\alpha_n) \\to \\min\\limits_{G_{n}, \\alpha_n}$, where\n",
    "    \n",
    "    $$ L_n(G, \\alpha) = \\sum\\limits_{i = 1}^m \\mathcal{L}\\bigl(y_i, f_{n - 1}(x_i) + \\alpha G(x_i)\\bigr) $$\n",
    "    \n",
    "    with the following method:\n",
    "    \n",
    "    \\begin{align}\n",
    "       g_i &= \\left. -\\frac{\\partial \\mathcal{L}(y_i, \\hat{y})}{\\partial \\hat{y}} \\right|_{\\hat{y} = G_{n - 1}(x_i)}\n",
    "          \\\\\n",
    "      G_n(x) &= \\arg\\min\\limits_{G \\in \\mathcal{A}}\\sum\\limits_{i = 1}^m \\bigl(G(x_i) - g_i\\bigr)^2\n",
    "          \\\\\n",
    "      \\alpha_n &= \\arg\\min\\limits_\\alpha L_n(G_n, \\alpha)\n",
    "          \\\\\n",
    "      f_n(x) &= f_{n - 1}(x) + \\alpha_n G_n(x)\n",
    "    \\end{align}\n",
    "    \n",
    "3. return $f_N(x) = f_0(x) + \\sum\\limits_{n = 1}^N \\alpha_n G_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49583dc",
   "metadata": {},
   "source": [
    "## Task 2. Gradient Boosting. [6 points]\n",
    "\n",
    "Assume that we've already found the optimal $G_n(x)$ at the step $n$ and we already have $f_{n-1}(x)$ from the previous step. Derive the expression for the **optimal value** of $\\alpha_n$ for the _MSE_ loss function $\\mathcal{L}(y, \\hat{y}) = (y - \\hat{y})^2$. Furthermore, explain what would happen to $\\alpha_n$ if the $|y_i - f_{n-1}(x_i)|$ values are close to 0 or significantly greater than $G_n(x_i)$ where $x_i$'s are data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54429a3",
   "metadata": {},
   "source": [
    "According to the method, $\\alpha_n$ is calculated as \n",
    "$$\n",
    "\\alpha_n = \\arg\\min_\\alpha \\left[L_n(G_n,\\alpha)\\right].\n",
    "$$\n",
    "We thus compute\n",
    "\\begin{align*}\n",
    "\\alpha_n &= \\arg\\min_\\alpha \\left[L_n(G_n,\\alpha)\\right] = \\arg\\min_\\alpha\\left[\\sum_{i=1}^m \\mathcal{L}\\left(y_i, f_{n-1}(x_i)+\\alpha G_n(x_i)\\right)\\right] \\\\\n",
    "&= \\arg\\min_{\\alpha}\\left[\\underbrace{\\sum_{i=1}^m (y_i - f_{n-1}(x_i) - \\alpha G_n(x_i))^2}_{\\doteq\\,F_n(\\alpha)}\\right] = \\arg\\min_{\\alpha}\\left[F_n(\\alpha)\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Towards finding a minimium of $F_n(\\alpha)$ with respect to $\\alpha$, one has to calculate the gradient $\\nabla_\\alpha F_n(\\alpha)$ as\n",
    "$$\n",
    "\\nabla_\\alpha F_n(\\alpha) = 2\\sum_{i=1}^m \\left(y_iG_n(x_i) - f_{n-1}(x_i)G_n(x_i) - \\alpha G_n(x_i)^2\\right).\n",
    "$$\n",
    "Setting $\\nabla_\\alpha F_n(\\alpha)\\bigr|_{\\alpha = \\alpha_n} \\overset{!}{=} 0$ and solving for $\\alpha_n$ leads to\n",
    "$$\n",
    "\\alpha_n = \\frac{\\sum_{i=1}^m G_n(x_i)\\left(y_i-f_{n-1}(x_i)\\right)}{\\sum_{j=1}^m G_n(x_j)^2},\n",
    "$$ \n",
    "where all involved quantities are known.\n",
    "\n",
    "Now, if $\\max(y_i - f_{n-1}(x_i)) \\doteq q \\rightarrow 0$, then we have $\\alpha_n \\approx  q\\sum_{i=1}^m\\frac{1}{G_n(x_i)} \\xrightarrow{q\\rightarrow 0} 0$. In this case, the weight of each additional model $G_n$ decreases with boosting steps; that is to say, overfitting is penalized.\n",
    "\n",
    "Similarly, if $q \\gg G_n(x_i)$ for all $i = 1,\\dots,m$, then we have $\\alpha_n \\approx  q\\sum_{i=1}^m\\frac{1}{G_n(x_i)} \\xrightarrow{q \\gg G_n(x_i)\\ \\forall i } \\infty$. In that case, the weight of each additional model $G_n$ blows up with every additional boosting step. In this way, overfitting is encouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a166f8f2",
   "metadata": {},
   "source": [
    "## Task 3. AdaBoost Weight Updates [3 points]\n",
    "\n",
    "$\\alpha_m$ parameter in AdaBoost update goes to infinity when ${err}_m$ goes to 0. What are the implications of this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d6e25",
   "metadata": {},
   "source": [
    "The error $err_m$ going to zero means that the model perfectly fits the training data. If that is the case, the prediction on the training data exactly matches the expected result. However, this means that one has also fitted possible noise in the training data, which in turn means that the model will badly generalize to new, unseen data. That is to say if $\\alpha_m \\rightarrow \\infty$, one has overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361327f0",
   "metadata": {},
   "source": [
    "## Task 4. Expectation Maximization (EM) [4 points]\n",
    "\n",
    "Prove that the following equation holds:\n",
    "\n",
    "$\\max\\left(\\sum\\limits_{i}\\ln\\left[p(x^{(i)}; \\theta)\\right]\\right) \\geq \\max\\left(\\sum\\limits_{i}\\sum\\limits_{z^{(i)}} Q_i(z^{(i)})\\ln\\left[\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}\\right]\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956e3bd",
   "metadata": {},
   "source": [
    "Let $Q_i(z)$ be a distribution over the $z$-variable, i.e. the conditions $\\sum_z Q_i(z) = 1$ and $Q_i(z) \\geq 0$ for all $i=1,\\dots,m$, where $m$ is the number of training samples.\n",
    "Now, for two random variables $x$ and $z$, the probability density $p(x)$ can be expressed by marginalization as\n",
    "$$\n",
    "p(x) = \\int_{\\mathbb{R}}p(x,z)\\,\\mathrm{d}z\n",
    "$$\n",
    "with the joint probaility density $p(x,z)$. The integral reduces to a sum, if $z$ is discrete. Hence, the probability density $p(x^{(i)};\\theta)$ can be marginalized with a further random variable $z^{(i)}$ as\n",
    "$$\n",
    "p(x^{(i)};\\theta) = \\sum_{z^{(i)}} p(x^{(i)},z^{(i)};\\theta).\n",
    "$$\n",
    "If we take the logarithm of the sum over all $i$, we obtain the expression\n",
    "$$\n",
    "\\sum_{i}\\ln\\left[p(x^{(i)};\\theta) \\right] = \\sum_{i}\\ln\\left[\\sum_{z^{(i)}} p(x^{(i)},z^{(i)};\\theta) \\right] = \\sum_{i}\\ln\\left[\\sum_{z^{(i)}} Q_i(z^{(i)})\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right].\n",
    "$$\n",
    "Note now, that the function $f(x) = \\ln(x)$ is concave for $x > 0$, because $\\frac{\\mathrm{d}^2 \\ln(x)}{\\mathrm{d}x^2} = -\\frac{1}{x^2} < 0$. For concave functions, Jensen's inequality states for a random variable $x \\sim p(x)$ that\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim p(x)}[f(x)] \\leq f(\\mathbb{E}_{x\\sim p(x)}[x])\n",
    "$$\n",
    "holds. Now, we have\n",
    "$$\n",
    "\\sum_{z^{(i)}} Q_i(z^{(i)})\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} = \\mathbb{E}_{z^{(i)}\\sim Q_i(z^{(i)})}\\left[ \\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right];\n",
    "$$\n",
    "moreover, by Jensen's inequality we have\n",
    "$$\n",
    "\\ln\\left[\\sum_{z^{(i)}} Q_i(z^{(i)})\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right] = \\ln\\left(\\mathbb{E}_{z^{(i)}\\sim Q_i(z^{(i)})}\\left[ \\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right]\\right) \\geq \\mathbb{E}_{z^{(i)}\\sim Q_i(z^{(i)})}\\left[\\ln\\left( \\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right) \\right] = \\sum_{z^{(i)}}Q_i(z^{(i)})\\ln\\left( \\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right).\n",
    "$$\n",
    "Also performing the sum over all $i$ yields \n",
    "$$\n",
    "\\sum_{i}\\ln\\left[p(x^{(i)};\\theta) \\right] \\geq \\sum_i \\sum_{z^{(i)}}Q_i(z^{(i)})\\ln\\left( \\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right).\n",
    "$$\n",
    "Since this inequality holds is not restricted to certain values of the domains for both expressions to the left and right of the inequality sign, the inequality must also hold for the maximum value of both sides:\n",
    "$$\n",
    "\\max\\left(   \\sum_{i}\\ln\\left[p(x^{(i)};\\theta) \\right] \\right) \\geq \\max\\left(  \\sum_i \\sum_{z^{(i)}}Q_i(z^{(i)})\\ln\\left[ \\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right]  \\right).\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
