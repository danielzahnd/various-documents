{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71e5a95",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cee62",
   "metadata": {},
   "source": [
    "## Task 1: Kernel theory [8 points]\n",
    "\n",
    "Let $K(x, x'): \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$ be a kernel, and $\\phi: \\mathbb{R}^n \\to \\mathbb{R}^m$ its **unknown** feature mapping. For $x, x' \\in \\mathbb{R}^n$ derive the squared Euclidean distance between $\\phi(x)$ and $\\phi(x')$ only in terms of $K(\\cdot, \\cdot)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e07a2f",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15374dff",
   "metadata": {},
   "source": [
    "Note, that the kernel is defined by\n",
    "$$\n",
    "K(x,x^\\prime) = \\phi(x)^\\top \\phi(x^\\prime) = \\sum_{i=1}^m \\phi_i(x)\\phi_i(x^\\prime),\n",
    "$$\n",
    "where $\\phi:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is the feature mapping. We can thus also express $K(x,x)$ and $K(x^\\prime,x^\\prime)$ as\n",
    "$$\n",
    "K(x,x) = \\sum_{i=1}^m \\phi_i(x)\\phi_i(x) = \\sum_{i=1}^m \\phi_i(x)^2, \\qquad K(x^\\prime,x^\\prime) = \\sum_{i=1}^m \\phi_i(x^\\prime)\\phi_i(x^\\prime) = \\sum_{i=1}^m \\phi_i(x^\\prime)^2.\n",
    "$$\n",
    "Now, the squared Euclidean distance between $\\phi(x)$ and $\\phi(x^\\prime)$ is given by\n",
    "\\begin{align*}\n",
    "\\|\\phi(x)-\\phi(x^\\prime)\\|^2 &= \\sum_{i=1}^m(\\phi_i(x)-\\phi_i(x^\\prime))^2 = \\sum_{i=1}^m \\left[   \\phi_i(x)^2 - 2\\phi_i(x)\\phi_i(x^\\prime) + \\phi_i(x^\\prime)   \\right] \\\\\n",
    "&= \\sum_{i=1}^m \\phi_i(x)^2 - 2\\sum_{i=1}^m\\phi_i(x)\\phi_i(x^\\prime) + \\sum_{i=1}^m \\phi(x^\\prime)^2 = K(x,x) - 2K(x,x^\\prime) + K(x^\\prime,x^\\prime).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f6720",
   "metadata": {},
   "source": [
    "## Task 2: SVM [9 points]\n",
    "\n",
    "Show that for a two-class SVM classifier trained on a linearly separable dataset $(x_i, y_i)_{i =1}^n$ the following upper bound on the leave-one-out-cross-validation error holds true:\n",
    "\n",
    "$$\n",
    "L_1OCV = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne f_i(x_i)) \\le \\frac{|SV|}{n},\n",
    "$$\n",
    "\n",
    "where $\\delta(c) = 1$ if $c$ is True and $\\delta(c) = 0$ if $c$ is False;  \n",
    "for all $i = 1, \\dots, n$ $f_i(x_i)$ is the SVM classifier fitted on the entire data without the observation $(x_i, y_i)$ and $|SV|$ is the number of support vectors of the SVM classifier fitted on the entire data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3f490",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff523e",
   "metadata": {},
   "source": [
    "Note, that if we show $\\sum_{i=1}^n\\delta(y_i\\neq f_i(x_i)) \\leq |SV|$, we have proven the inequality. We can expand the sum on the left hand side to\n",
    "$$\n",
    "\\sum_{i=1}^n\\delta(y_i\\neq f_i(x_i)) = \\delta(y_1\\neq f_1(x_1)) + \\dots + \\delta(y_n \\neq f_n(x_n)).\n",
    "$$\n",
    "Now say that a model $f_j$ was trained on the dataset ${(x_i,y_i)}_{i=1}^m  \\backslash (x_j,y_j)$. If we assume, that $(x_j,y_j)$ would be a support vector if we included it in the training data, then the decision boundary would possibly change; if $(x_j,y_j)$ however would not be a support vector if included in the training dataset, nothing changed. Therefore, we can conclude\n",
    "$$\n",
    "\\delta(y_i\\neq f_i(x_i)) = \\begin{cases} \\text{Possibly 1}, &\\quad \\text{if $(x_i,y_i)$ would be support vector}, \\\\\n",
    "\\text{Always 0}, &\\quad \\text{otherwise}\\end{cases}.\n",
    "$$ \n",
    "Therefore, we have $\\max{\\left(\\sum_{i=1}^n\\delta(y_i\\neq f_i(x_i))\\right)} = |SV|$. \n",
    "That is to say, the sum is always equal or less than the number $|SV|$ of support vectors of a classifier fitted on the whole dataset. Thus we have\n",
    "$$\n",
    "\\sum_{i=1}^n\\delta(y_i\\neq f_i(x_i)) = \\delta(y_1\\neq f_1(x_1)) + \\dots + \\delta(y_n \\neq f_n(x_n)) \\leq |SV|\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "L_1OCV = \\frac{1}{n}\\sum_{i=1}^n\\delta(y_i\\neq f_i(x_i)) \\leq \\frac{|SV|}{n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb203f",
   "metadata": {},
   "source": [
    "## Task 3. Decision Tree Leaves [6 points]\n",
    "\n",
    "Consider a leaf of a decision tree that consists of object-label pairs $(x_{1}, y_{1}), \\dots, (x_{n}, y_{n})$.\n",
    "\n",
    "The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefc354",
   "metadata": {},
   "source": [
    "Find the **optimal prediction** in the leaf, for a regression tree, i.e. $y_{i} \\in \\mathbb{R}$, and squared percentage error loss $\\mathcal{L}(y, \\hat{y}) = \\cfrac{\\left(y - \\hat{y} \\right)^{2}}{y^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa5b6e",
   "metadata": {},
   "source": [
    "### Your solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e4a9",
   "metadata": {},
   "source": [
    "Given the loss function $\\mathcal{L}(y,\\hat{y})$, we can write the loss on the whole leaf as\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}) = \\sum_{i=1}^n\\frac{(y_i-\\hat{y})^2}{y_i^2}.\n",
    "$$\n",
    "In order to find the optimal prediction $\\hat{y}$ on the leaf, we have to minimize $\\mathcal{L}(\\hat{y})$. This means taking the derivative with respect to $\\hat{y}$, setting the equation to zero and solving for $\\hat{y}$.\n",
    "Performing this yields\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d}\\hat{y}}\\mathcal{L}(\\hat{y}) = -\\sum_{i=1}^n\\frac{2(y_i-\\hat{y})}{y_i^2} \\overset{!}{=} 0.\n",
    "\\end{align*}\n",
    "Rearranging terms, we get the result\n",
    "$$\n",
    "\\hat{y} = \\frac{\\sum_{i=1}^n y_i^{-1}}{\\sum_{j=1}^n y_j^{-2}}.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
